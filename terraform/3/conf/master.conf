#cloud-config
coreos:
  etcd2:
    discovery: ${etcd_discovery_url}
    advertise-client-urls: http://$private_ipv4:2379,http://$private_ipv4:4001
    initial-advertise-peer-urls: http://$private_ipv4:2380
    listen-client-urls: http://0.0.0.0:2379,http://0.0.0.0:4001
    listen-peer-urls: http://$private_ipv4:2380
  fleet:
    public-ip: $private_ipv4
  units:
    - name: "etcd2.service"
      command: "start"
      enable: true
    - name: "bootstrap-master.service"
      command: start
      enable: false
      content: |
        [Unit]
        Requires=etcd2.service
        After=etcd2.service
        Before=flanneld.service
        [Service]
        Type=oneshot
        ExecStart=/root/bootstrap/bootstrap-master.sh
        Restart=no
    - name: "bootstrap-namespaces.service"
      command: start
      enable: false
      content: |
        [Unit]
        Requires=kubelet.service
        After=kubelet.service
        [Service]
        Type=oneshot
        ExecStart=/root/bootstrap/bootstrap-namespaces.sh
        Restart=no
    - name: "fleet.service"
      command: "start"
      enable: true
    - name: "flanneld.service"
      command: "start"
      enable: true
      drop-ins:
        - name: 40-ExecStartPre-symlink.conf
          content: |
            [Service]
            ExecStartPre=/usr/bin/ln -sf /etc/flannel/options.env /run/flannel/options.env
    - name: "docker.service"
      command: start
      enable: true
      drop-ins:
        - name: 40-Flannel.conf
          content: |
            [Unit]
            Requires=flanneld.service
            Requires=calico-node.service
            After=flanneld.service
            After=calico-node.service
    - name: "kubelet.service"
      command: start
      enable: true
      content: |
        [Unit]
        Required=docker.service
        After=docker.service
        After=calico-node.service

        [Service]
        ExecStartPre=/usr/bin/mkdir -p /etc/kubernetes/manifests
        ExecStartPre=/usr/bin/mkdir -p /var/log/containers

        Environment=KUBELET_VERSION=${kubernetes_version}
        Environment="RKT_OPTS=--volume var-log,kind=host,source=/var/log \
          --mount volume=var-log,target=/var/log \
          --volume dns,kind=host,source=/etc/resolv.conf \
          --mount volume=dns,target=/etc/resolv.conf"

        ExecStart=/usr/lib/coreos/kubelet-wrapper \
          --api-servers=http://127.0.0.1:8080 \
          --network-plugin-dir=/etc/kubernetes/cni/net.d \
          --network-plugin=cni \
          --register-schedulable=false \
          --allow-privileged=true \
          --config=/etc/kubernetes/manifests \
          --hostname-override=$private_ipv4 \
          --cluster-dns=${dns_service_ip} \
          --cluster-domain=cluster.local
        Restart=always
        RestartSec=10

        [Install]
        WantedBy=multi-user.target
    - name: "calico-node.service"
      command: start
      enable: true
      content: |
        [Unit]
        Description=Calico per-host agent
        Requires=network-online.target
        After=network-online.target

        [Service]
        Slice=machine.slice
        Environment=CALICO_DISABLE_FILE_LOGGING=true
        Environment=HOSTNAME=$private_ipv4
        Environment=IP=$private_ipv4
        Environment=FELIX_FELIXHOSTNAME=$private_ipv4
        Environment=CALICO_NETWORKING=false
        Environment=NO_DEFAULT_POOLS=true
        Environment=ETCD_ENDPOINTS=http://$private_ipv4:2379
        ExecStart=/usr/bin/rkt run --inherit-env --stage1-from-dir=stage1-fly.aci \
        --volume=modules,kind=host,source=/lib/modules,readOnly=false \
        --mount=volume=modules,target=/lib/modules \
        --volume=dns,kind=host,source=/etc/resolv.conf,readOnly=true \
        --mount=volume=dns,target=/etc/resolv.conf \
        --trust-keys-from-https quay.io/calico/node:v0.19.0

        KillMode=mixed
        Restart=always
        TimeoutStartSec=0

        [Install]
        WantedBy=multi-user.target
write_files:
  - path: "/etc/flannel/options.env"
    permissions: "0644"
    owner: "root"
    content: |
      FLANNELD_IFACE=$private_ipv4
      FLANNELD_ETCD_ENDPOINTS=http://$private_ipv4:2379
  - path: "/root/bootstrap/api-openssl.cnf"
    permissions: "0644"
    owner: "root"
    content: |
      [req]
      req_extensions = v3_req
      distinguished_name = req_distinguished_name
      [req_distinguished_name]
      [ v3_req ]
      basicConstraints = CA:FALSE
      keyUsage = nonRepudiation, digitalSignature, keyEncipherment
      subjectAltName = @alt_names
      [alt_names]
      DNS.1 = kubernetes
      DNS.2 = kubernetes.default
      DNS.3 = kubernetes.default.svc
      DNS.4 = kubernetes.default.svc.cluster.local
      IP.1 = ${kubernetes_service_ip}
      IP.2 = $private_ipv4
  - path: "/root/bootstrap/worker-openssl.cnf"
    permissions: "0644"
    owner: "root"
    content: |
      [req]
      req_extensions = v3_req
      distinguished_name = req_distinguished_name
      [req_distinguished_name]
      [ v3_req ]
      basicConstraints = CA:FALSE
      keyUsage = nonRepudiation, digitalSignature, keyEncipherment
      subjectAltName = @alt_names
      [alt_names]
      IP.1 = $ENV::WORKER_IP
  - path: "/root/bootstrap/bootstrap-master.sh"
    permissions: "0755"
    owner: "root"
    content: |
      #!/usr/bin/env bash
      set -e
      set -u

      echo "Creating keys"

      cd /root/bootstrap

      # generate CA
      openssl genrsa -out ca-key.pem 2048
      openssl req -x509 -new -nodes -key ca-key.pem -days 10000 -out ca.pem -subj "/CN=ca"

      # generate certs for the API server
      openssl genrsa -out apiserver-key.pem 2048
      openssl req -new -key apiserver-key.pem -out apiserver.csr -subj "/CN=kube-apiserver" -config api-openssl.cnf
      openssl x509 -req -in apiserver.csr -CA ca.pem -CAkey ca-key.pem -CAcreateserial -out apiserver.pem -days 365 -extensions v3_req -extfile api-openssl.cnf

      # generate certs for the cluster admin
      openssl genrsa -out admin-key.pem 2048
      openssl req -new -key admin-key.pem -out admin.csr -subj "/CN=kube-admin"
      openssl x509 -req -in admin.csr -CA ca.pem -CAkey ca-key.pem -CAcreateserial -out admin.pem -days 365

      # change permissions
      chmod 700 .
      chmod 600 *.pem *.csr

      # copy them to the appropriate location
      mkdir -p /etc/kubernetes/ssl
      cp ca.pem /etc/kubernetes/ssl/ca.pem
      cp apiserver.pem /etc/kubernetes/ssl/apiserver.pem
      cp apiserver-key.pem /etc/kubernetes/ssl/apiserver-key.pem
      cp admin.pem /etc/kubernetes/ssl/admin.pem
      cp admin-key.pem /etc/kubernetes/ssl/admin-key.pem

      # add config for flannel
      echo "Add config for flannel"

      curl -X PUT -d "value={\"Network\":\"${pod_network}\",\"Backend\":{\"Type\":\"vxlan\"}}" "http://$private_ipv4:2379/v2/keys/coreos.com/network/config"

  - path: "/root/bootstrap/bootstrap-namespaces.sh"
    permissions: "0755"
    owner: "root"
    content: |
      #!/usr/bin/env bash
      set -e
      set -u

      until curl --silent http://127.0.0.1:8080/version 2>/dev/null | grep gitVersion > /dev/null; do
        echo "Waiting for kube server to start up..."
        sleep 10
      done

      echo "Creating namespaces"

      curl -H "Content-Type: application/json" -XPOST -d'{"apiVersion":"v1","kind":"Namespace","metadata":{"name":"kube-system"}}' "http://127.0.0.1:8080/api/v1/namespaces"
      curl -H "Content-Type: application/json" -XPOST -d'{"apiVersion":"v1","kind":"Namespace","metadata":{"name":"calico-system"}}' "http://127.0.0.1:8080/api/v1/namespaces"
  - path: "/root/bootstrap/generate-worker-cert.sh"
    permissions: "0755"
    owner: "root"
    content: |
      #!/usr/bin/env bash
      set -e
      set -u

      cd /root/bootstrap

      WORKER_FQDN=$1
      WORKER_IP=$2

      openssl genrsa -out $WORKER_FQDN-worker-key.pem 2048
      WORKER_IP=$WORKER_IP openssl req -new -key $WORKER_FQDN-worker-key.pem -out $WORKER_FQDN-worker.csr -subj "/CN=$WORKER_FQDN" -config worker-openssl.cnf
      WORKER_IP=$WORKER_IP openssl x509 -req -in $WORKER_FQDN-worker.csr -CA ca.pem -CAkey ca-key.pem -CAcreateserial -out $WORKER_FQDN-worker.pem -days 365 -extensions v3_req -extfile worker-openssl.cnf

      chown core:core $WORKER_FQDN-worker-key.pem
      chown core:core $WORKER_FQDN-worker.pem

      mv $WORKER_FQDN-worker-key.pem /home/core
      mv $WORKER_FQDN-worker.pem /home/core

      cp ca.pem /home/core/$WORKER_FQDN-ca.pem
      chown core:core /home/core/$WORKER_FQDN-ca.pem
      chmod 755 /home/core/$WORKER_FQDN-ca.pem

  - path: "/etc/kubernetes/manifests/kube-apiserver.yaml"
    permissions: "0644"
    owner: "root"
    content: |
      apiVersion: v1
      kind: Pod
      metadata:
        name: kube-apiserver
        namespace: kube-system
      spec:
        hostNetwork: true
        containers:
        - name: kube-apiserver
          image: quay.io/coreos/hyperkube:${kubernetes_version}
          command:
          - /hyperkube
          - apiserver
          - --bind-address=0.0.0.0
          - --etcd-servers=http://$private_ipv4:2379
          - --allow-privileged=true
          - --service-cluster-ip-range=${service_ip_range}
          - --secure-port=443
          - --advertise-address=$private_ipv4
          - --admission-control=NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,ResourceQuota
          - --tls-cert-file=/etc/kubernetes/ssl/apiserver.pem
          - --tls-private-key-file=/etc/kubernetes/ssl/apiserver-key.pem
          - --client-ca-file=/etc/kubernetes/ssl/ca.pem
          - --service-account-key-file=/etc/kubernetes/ssl/apiserver-key.pem
          - --runtime-config=extensions/v1beta1=true,extensions/v1beta1/networkpolicies=true
          ports:
          - containerPort: 443
            hostPort: 443
            name: https
          - containerPort: 8080
            hostPort: 8080
            name: local
          volumeMounts:
          - mountPath: /etc/kubernetes/ssl
            name: ssl-certs-kubernetes
            readOnly: true
          - mountPath: /etc/ssl/certs
            name: ssl-certs-host
            readOnly: true
        volumes:
        - hostPath:
            path: /etc/kubernetes/ssl
          name: ssl-certs-kubernetes
        - hostPath:
            path: /usr/share/ca-certificates
          name: ssl-certs-host
  - path: "/etc/kubernetes/manifests/kube-proxy.yaml"
    permissions: "0644"
    owner: "root"
    content: |
      apiVersion: v1
      kind: Pod
      metadata:
        name: kube-proxy
        namespace: kube-system
      spec:
        hostNetwork: true
        containers:
        - name: kube-proxy
          image: quay.io/coreos/hyperkube:${kubernetes_version}
          command:
          - /hyperkube
          - proxy
          - --master=http://127.0.0.1:8080
          - --proxy-mode=iptables
          securityContext:
            privileged: true
          volumeMounts:
          - mountPath: /etc/ssl/certs
            name: ssl-certs-host
            readOnly: true
        volumes:
        - hostPath:
            path: /usr/share/ca-certificates
          name: ssl-certs-host
  - path: "/etc/kubernetes/manifests/kube-controller-manager.yaml"
    permissions: "0644"
    owner: "root"
    content: |
      apiVersion: v1
      kind: Pod
      metadata:
        name: kube-controller-manager
        namespace: kube-system
      spec:
        hostNetwork: true
        containers:
        - name: kube-controller-manager
          image: quay.io/coreos/hyperkube:${kubernetes_version}
          command:
          - /hyperkube
          - controller-manager
          - --master=http://127.0.0.1:8080
          - --leader-elect=true
          - --service-account-private-key-file=/etc/kubernetes/ssl/apiserver-key.pem
          - --root-ca-file=/etc/kubernetes/ssl/ca.pem
          livenessProbe:
            httpGet:
              host: 127.0.0.1
              path: /healthz
              port: 10252
            initialDelaySeconds: 15
            timeoutSeconds: 1
          volumeMounts:
          - mountPath: /etc/kubernetes/ssl
            name: ssl-certs-kubernetes
            readOnly: true
          - mountPath: /etc/ssl/certs
            name: ssl-certs-host
            readOnly: true
        volumes:
        - hostPath:
            path: /etc/kubernetes/ssl
          name: ssl-certs-kubernetes
        - hostPath:
            path: /usr/share/ca-certificates
          name: ssl-certs-host
  - path: "/etc/kubernetes/manifests/kube-scheduler.yaml"
    permissions: "0644"
    owner: "root"
    content: |
      apiVersion: v1
      kind: Pod
      metadata:
        name: kube-scheduler
        namespace: kube-system
      spec:
        hostNetwork: true
        containers:
        - name: kube-scheduler
          image: quay.io/coreos/hyperkube:${kubernetes_version}
          command:
          - /hyperkube
          - scheduler
          - --master=http://127.0.0.1:8080
          - --leader-elect=true
          livenessProbe:
            httpGet:
              host: 127.0.0.1
              path: /healthz
              port: 10251
            initialDelaySeconds: 15
            timeoutSeconds: 1
  - path: "/etc/kubernetes/manifests/policy-controller.yaml"
    permissions: "0644"
    owner: "root"
    content: |
      apiVersion: v1
      kind: Pod
      metadata:
        name: calico-policy-controller
        namespace: calico-system
      spec:
        hostNetwork: true
        containers:
          # The Calico policy controller.
          - name: k8s-policy-controller
            image: calico/kube-policy-controller:v0.2.0
            env:
              - name: ETCD_ENDPOINTS
                value: "http://$private_ipv4:2379"
              - name: K8S_API
                value: "http://127.0.0.1:8080"
              - name: LEADER_ELECTION
                value: "true"
          # Leader election container used by the policy controller.
          - name: leader-elector
            image: quay.io/calico/leader-elector:v0.1.0
            imagePullPolicy: IfNotPresent
            args:
              - "--election=calico-policy-election"
              - "--election-namespace=calico-system"
              - "--http=127.0.0.1:4040"
  - path: "/etc/kubernetes/cni/net.d/10-calico.conf"
    permissions: "0644"
    owner: "root"
    content: |
      {
          "name": "calico",
          "type": "flannel",
          "delegate": {
              "type": "calico",
              "etcd_endpoints": "http://$private_ipv4:2379",
              "log_level": "none",
              "log_level_stderr": "info",
              "hostname": "$private_ipv4",
              "policy": {
                  "type": "k8s",
                  "k8s_api_root": "http://127.0.0.1:8080/api/v1/"
              }
          }
      }
